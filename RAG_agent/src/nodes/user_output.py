"""User output node for streaming final responses to the user as LLM interface.

This module provides the ONLY node responsible for displaying output to users
in a clean LLM interface format. It formats responses as markdown content and
maintains clean separation between business logic and presentation layer.

Design Philosophy:
- Single responsibility: display output to user in LLM interface format
- No business logic or state mutations
- Configurable streaming behavior via settings
- Formats output as markdown for better readability
- Easy to test and swap implementations (console â†’ web â†’ CLI)
"""

from __future__ import annotations

import logging
import time
from typing import Any, Dict

from src.graph.state import AgentState
from settings import ENABLE_STREAMING

__all__ = [
    "user_output_node",
]

logger = logging.getLogger(__name__)


def _format_as_markdown_response(response_text: str) -> str:
    """
    Format the response text as a clean markdown document.

    Args:
        response_text: Raw response text from the LLM

    Returns:
        Formatted markdown string
    """
    if not response_text:
        return "*No response generated*"

    # Split into lines for processing
    lines = response_text.strip().split('\n')

    # Clean up the response
    formatted_lines = []

    for line in lines:
        line = line.rstrip()
        if not line:
            formatted_lines.append("")
            continue

        # Preserve existing markdown formatting
        # Add basic formatting if needed
        if not any(line.startswith(prefix) for prefix in ['#', '-', '*', '1.', '```', '>']):
            # If it looks like a paragraph, keep it as is
            pass

        formatted_lines.append(line)

    # Join back and add final formatting
    markdown_content = '\n'.join(formatted_lines).strip()

    # Add timestamp and metadata
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    header = f"""# Response Generated

**Timestamp:** {timestamp}
**Status:** Complete

---

"""

    footer = """

---

*Response generated by Hay's RAG Assistant*
"""

    return header + markdown_content + footer


def user_output_node(state: AgentState) -> Dict[str, Any]:
    """
    Display final response to user as formatted LLM interface with markdown.

    This is the ONLY node that should handle user-facing output and streaming.
    It takes the final_response from state, formats it as a clean markdown document,
    and displays it in an LLM interface style according to ENABLE_STREAMING setting.

    Output Format:
    - Header with timestamp and status
    - Formatted markdown content
    - Footer with attribution

    Streaming behavior:
    - If ENABLE_STREAMING=True: Character-by-character streaming for realistic LLM feel
    - If ENABLE_STREAMING=False: Complete markdown document displayed immediately

    Input state:
        - final_response: str (the complete response text)

    Output state:
        - No state updates (terminal presentation node)

    Args:
        state: Current graph state containing final_response

    Returns:
        Empty dict (no state updates needed)

    Example:
        >>> state = {"final_response": "LangGraph supports checkpointing..."}
        >>> user_output_node(state)
        ============================================================
        ðŸ¤– LLM RESPONSE
        ============================================================
        # Response Generated

        **Timestamp:** 2024-01-01 12:00:00
        **Status:** Complete

        ---

        LangGraph supports checkpointing...

        ---

        *Response generated by Hay's RAG Assistant*
        ============================================================
        {}
    """
    final_response = state.get("final_response")

    if not final_response:
        logger.warning("No final_response found in state, skipping user output")
        return {}

    # Ensure final_response is a string (handle LangChain edge cases)
    if isinstance(final_response, list):
        logger.warning("final_response is a list, converting to string")
        final_response = ' '.join(str(item) for item in final_response)
    elif not isinstance(final_response, str):
        final_response = str(final_response)

    # Format response as markdown document
    markdown_response = _format_as_markdown_response(final_response)

    if ENABLE_STREAMING:
        # Stream character-by-character for realistic LLM interface feel
        logger.debug("Streaming markdown response to user...")
        print("\n" + "="*60)
        print("ðŸ¤– LLM RESPONSE")
        print("="*60)

        # Stream the markdown content
        for char in markdown_response:
            print(char, end="", flush=True)
            # Very small delay for realistic streaming
            time.sleep(0.005)

        print("\n" + "="*60)
        logger.info("Markdown response streaming completed")

    else:
        # Print complete markdown response immediately
        logger.debug("Displaying markdown response to user...")
        print("\n" + "="*60)
        print("ðŸ¤– LLM RESPONSE")
        print("="*60)
        print(markdown_response)
        print("="*60)
        logger.info("Markdown response displayed")

    # No state updates needed - this is a terminal presentation node
    return {}
